{
  "hash": "e9a1650f7c9bb4b4b46a8e46fde14753",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"hw9\"\nsubtitle: \"로지스틱 회귀분석 개념\"\nauthor: \"정은서\"\neditor: visual\njupyter: python3\n---\n\n\n\n\n# 문제1 : 데이터를 로드하고, 로지스틱 회귀모델을 적합하고, 회귀 표를 작성하세요.\n\n::: {#f3d87635 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom scipy.stats import chi2, norm\n\ndf = pd.read_csv('../../../lsbigdata-project1/leukemia_remission/leukemia_remission.txt', delim_whitespace= True) \ntrain = df.drop(columns=('REMISS'))\n\nmodel = sm.formula.logit(\"REMISS ~ CELL + SMEAR + INFIL + LI + BLAST + TEMP\", data=df).fit()\nprint(model.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOptimization terminated successfully.\n         Current function value: 0.399886\n         Iterations 10\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                 REMISS   No. Observations:                   27\nModel:                          Logit   Df Residuals:                       20\nMethod:                           MLE   Df Model:                            6\nDate:                Tue, 10 Sep 2024   Pseudo R-squ.:                  0.3718\nTime:                        10:40:00   Log-Likelihood:                -10.797\nconverged:                       True   LL-Null:                       -17.186\nCovariance Type:            nonrobust   LLR p-value:                   0.04670\n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     64.2581     74.965      0.857      0.391     -82.670     211.187\nCELL          30.8301     52.135      0.591      0.554     -71.353     133.013\nSMEAR         24.6863     61.526      0.401      0.688     -95.903     145.275\nINFIL        -24.9745     65.281     -0.383      0.702    -152.923     102.974\nLI             4.3605      2.658      1.641      0.101      -0.849       9.570\nBLAST         -0.0115      2.266     -0.005      0.996      -4.453       4.430\nTEMP        -100.1734     77.753     -1.288      0.198    -252.567      52.220\n==============================================================================\n\nPossibly complete quasi-separation: A fraction 0.11 of observations can be\nperfectly predicted. This might indicate that there is complete\nquasi-separation. In this case some parameters will not be identified.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2844\\1794366257.py:7: FutureWarning:\n\nThe 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n\n```\n:::\n:::\n\n\n####  \n\n# 문제2 : 해당 모델은 통계적으로 유의한가요? 그 이유를 검정통계량를 사용해서 설명하시오\n\n::: {#bed270cd .cell execution_count=2}\n``` {.python .cell-code}\n# 검정통계량 : −2(ℓ(𝛽)̂ (0) − ℓ(𝛽)̂ )  =  -2*(-17.186+10.797)  = 12.779\n1 - chi2.cdf(12.779, df=6)  # 0.0467\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\n0.0466828104726148\n```\n:::\n:::\n\n\n결론 : LLR p-value: 0.0467 < 유의수준 0.05보다 작으니까 통계적으로 유의하다고 할 수 있다.  \n\n\n####  \n\n# 문제3 : 유의수준이 0.2를 기준으로 통계적으로 유의한 변수는 몇개이며, 어느 변수 인가요?\n\n::: {#bbe08387 .cell execution_count=3}\n``` {.python .cell-code}\nprint(model.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                 REMISS   No. Observations:                   27\nModel:                          Logit   Df Residuals:                       20\nMethod:                           MLE   Df Model:                            6\nDate:                Tue, 10 Sep 2024   Pseudo R-squ.:                  0.3718\nTime:                        10:40:00   Log-Likelihood:                -10.797\nconverged:                       True   LL-Null:                       -17.186\nCovariance Type:            nonrobust   LLR p-value:                   0.04670\n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     64.2581     74.965      0.857      0.391     -82.670     211.187\nCELL          30.8301     52.135      0.591      0.554     -71.353     133.013\nSMEAR         24.6863     61.526      0.401      0.688     -95.903     145.275\nINFIL        -24.9745     65.281     -0.383      0.702    -152.923     102.974\nLI             4.3605      2.658      1.641      0.101      -0.849       9.570\nBLAST         -0.0115      2.266     -0.005      0.996      -4.453       4.430\nTEMP        -100.1734     77.753     -1.288      0.198    -252.567      52.220\n==============================================================================\n\nPossibly complete quasi-separation: A fraction 0.11 of observations can be\nperfectly predicted. This might indicate that there is complete\nquasi-separation. In this case some parameters will not be identified.\n```\n:::\n:::\n\n\nP>|z|가 0.2보다 작은 LI, TEMP가 유의하다\n\n####  \n\n\n# 문제4 : 다음 환자에 대한 오즈는 얼마인가요?\n#### CELL (골수의 세포성): 65% , SMEAR (골수편의 백혈구 비율): 45% , INFIL (골수의 백혈병 세포 침투 비율): 55% , LI (골수 백혈병 세포의 라벨링 인덱스): 1.2 , BLAST (말초혈액의 백혈병 세포 수): 1.1세포/μL , TEMP (치료 시작 전 최고 체온): 0.9\n\n::: {#f4b58f1c .cell execution_count=4}\n``` {.python .cell-code}\nodds = np.exp( 64.2581 + 30.8301*0.65 + 24.6863 * 0.45  - 24.9745 * 0.55 + 4.3605 * 1.2 - 0.0115*1.1 - 100.1734 * 0.9)\nprint(odds) # 오즈 : 0.03817459641135519\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.03817459641135519\n```\n:::\n:::\n\n\n####  \n\n# 문제5 : 위 환자의 혈액에서 백혈병 세포가 관측되지 않은 확률은 얼마인가요?\n\n::: {#35c5b996 .cell execution_count=5}\n``` {.python .cell-code}\nodds / (1+odds)  # 0.03677088280074742\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n0.03677088280074742\n```\n:::\n:::\n\n\n#### \n\n\n#  문제 6 : TEMP 변수의 계수는 얼마이며, 해당 계수를 사용해서 TEMP 변수가 백혈병 치료에 대한 영향을 설명하시오.\nTEMP 변수의 계수 : -100.1734\ne^(-100.1734) = 3.13e-44 는 0에 가까운 값입니다. 이는 체온이 1단위 상승할 때 백혈병 세포가 관측되지 않을 확률이 (오즈비만큼 변동)거의 없어지는 것을 의미 ->  온도가 높아질수록 백혈병 세포가 관측될 확률 높아짐.\n\nTEMP 변수 1 단위 증가하면 로그오즈가 100.1734 감소하는데, 오즈비는 (e^-100.1734)배 감소한다. TEMP가 1단위 올라가면, 오즈비가 (e^-100.1734)배 감소. 관측 불가 확률이 감소한다. 즉 관측될 확률이 증가한다. 따라서 백혈병 치료를 해야한다.\n\n\n\n# 문제 7 : CELL 변수의 99% 오즈비에 대한 신뢰구간을 구하시오.\nCELL 변수의 베타에 대한 99% 신뢰구간 : (베타_hat - z(0.005)*SE , 베타_hat + z(0.005)*SE)\nCELL 변수의 오즈비에 대한 99% 신뢰구간 : (exp(베타_hat - z(0.005)*SE) , exp(베타_hat + z(0.005)*SE))\n\n::: {#6f6862e0 .cell execution_count=6}\n``` {.python .cell-code}\nz0005 = norm.ppf(0.995, loc=0, scale=1)  # 2.5758293035489004\n# CELL 변수의 베타에 대한 99% 신뢰구간 : (-103.4607607405219, 165.12096074052192\n30.8301 - 52.135*z0005 , 30.8301 + 52.135*z0005 \n# CELL 변수의 오즈비에 대한 99% 신뢰구간 : (1.1683218982002717e-45, 5.141881884993857e+71)\nnp.exp(30.8301 - 52.135*z0005) , np.exp(30.8301 + 52.135*z0005)  \n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n(1.1683218982002717e-45, 5.141881884993857e+71)\n```\n:::\n:::\n\n\nCELL 변수의 베타에 대한 99% 신뢰구간 : (-103.4607607405219, 165.12096074052192\nCELL 변수의 오즈비에 대한 99% 신뢰구간 : (1.1683218982002717e-45, 5.141881884993857e+71)\n\n\n\n#### \n\n# 문제 8 : 주어진 데이터에 대하여 로지스틱 회귀 모델의 예측 확률을 구한 후, 50% 이상인 경우 1로 처리하여, 혼동 행렬를 구하시오.\n\n::: {#4a4d4997 .cell execution_count=7}\n``` {.python .cell-code}\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\ny_pred = model.predict(train)\nresult = pd.DataFrame({'y_pred' : y_pred})\nresult['result'] = np.where(result['y_pred']>=0.5, 1,0)\n\nconf_mat = confusion_matrix(y_true = df['REMISS'], y_pred = result['result'], labels=[1,0])\np = ConfusionMatrixDisplay(confusion_matrix = conf_mat, display_labels = ('관측불가_1', '관측가능_0'))\nplt.rcParams['font.family'] = 'Malgun Gothic'\np.plot(cmap=\"Blues\")\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-1.png){width=551 height=430}\n:::\n:::\n\n\n#### \n\n# 문제 9 : 해당 모델의 Accuracy는 얼마인가요?\n\n::: {#7f78f971 .cell execution_count=8}\n``` {.python .cell-code}\n# 방법1 \nprint((5+15)/(5+3+4+15))  # 0.7407407407407407\n\n# 방법2\nfrom sklearn.metrics import accuracy_score, f1_score\nprint(accuracy_score(df['REMISS'], result['result'])) # 0.7407407407407407\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.7407407407407407\n0.7407407407407407\n```\n:::\n:::\n\n\n#### \n\n# 문제10 : 해당 모델의 F1 Score를 구하세요.\n\n::: {#3022c7d3 .cell execution_count=9}\n``` {.python .cell-code}\n# 방법1\nprecision = 5/(5+3)\nrecall = 5/(5+4)\nprint(2 / (1/precision + 1/recall))  # 0.5882352941176471\n\n# 방법2\nprint(f1_score(df['REMISS'], result['result']))  # 0.5882352941176471\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.5882352941176471\n0.5882352941176471\n```\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}